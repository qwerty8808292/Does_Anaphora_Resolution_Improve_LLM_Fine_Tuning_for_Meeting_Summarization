{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 19749,
     "status": "ok",
     "timestamp": 1750043080795,
     "user": {
      "displayName": "YI-CHUN LO",
      "userId": "03461610637064647812"
     },
     "user_tz": -60
    },
    "id": "LfpZcsvKXzc_"
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    GenerationConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from transformers.generation.utils import EncoderDecoderCache\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from scipy.stats import wilcoxon, ttest_rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1750043081332,
     "user": {
      "displayName": "YI-CHUN LO",
      "userId": "03461610637064647812"
     },
     "user_tz": -60
    },
    "id": "pejXilMmX03-"
   },
   "outputs": [],
   "source": [
    "seed = 413\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1750043081335,
     "user": {
      "displayName": "YI-CHUN LO",
      "userId": "03461610637064647812"
     },
     "user_tz": -60
    },
    "id": "fVPsdHy7YKx1"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4JSt0nSYLpl"
   },
   "outputs": [],
   "source": [
    "inputs = {\"raw\": \"dialogue\", \"resolved\": \"resolved_text\"}\n",
    "data = load_from_disk(\"samsum_new\")\n",
    "model_checkpoint = \"facebook/bart-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1750043086238,
     "user": {
      "displayName": "YI-CHUN LO",
      "userId": "03461610637064647812"
     },
     "user_tz": -60
    },
    "id": "oQXgZqlcYc5V"
   },
   "outputs": [],
   "source": [
    "def preprocess(data, input):\n",
    "    inputs = data[input]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    labels = tokenizer(text_target=data[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F73Y-i5LYj0V"
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    predictions = np.array(predictions)\n",
    "    predictions = predictions.astype(np.int64)\n",
    "    predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n",
    "    predictions = predictions.tolist()\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    if isinstance(labels, tuple):\n",
    "        labels = labels[0]\n",
    "    labels = np.array(labels)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = labels.astype(np.int64).tolist()\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjAUa225YrNc"
   },
   "outputs": [],
   "source": [
    "all_metrics = OrderedDict()\n",
    "\n",
    "trainer_raw = None\n",
    "trainer_resolved = None\n",
    "\n",
    "\n",
    "for version, input_ in inputs.items():\n",
    "    tokenized_data = data.map(\n",
    "        lambda x: preprocess(x, input_),\n",
    "        batched=True,\n",
    "        remove_columns=data[\"train\"].column_names\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "    model.config.use_cache = False\n",
    "    for name, param in model.named_parameters():\n",
    "      if any(f\"encoder.layers.{i}.\" in name for i in range(3)):\n",
    "          param.requires_grad = False\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"/content/bart-large-{version}-finetuned\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=0.0001,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=3,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True,\n",
    "        push_to_hub=False,\n",
    "        logging_dir=f\"/content/logs-{version}\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        logging_steps=500,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_data[\"train\"],\n",
    "        eval_dataset=tokenized_data[\"validation\"],\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    if version == \"raw\":\n",
    "        trainer_raw = trainer\n",
    "    else:\n",
    "        trainer_resolved = trainer\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"/content/bart-large-{version}-finetuned\")\n",
    "\n",
    "    all_metrics[version] = trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1750045382651,
     "user": {
      "displayName": "YI-CHUN LO",
      "userId": "03461610637064647812"
     },
     "user_tz": -60
    },
    "id": "bT81fhcdUiR-",
    "outputId": "1bb903d4-1502-430d-dc77-57dfd962980b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for raw input:\n",
      "Loss      : 1.4854\n",
      "ROUGE1    : 49.5428\n",
      "ROUGE2    : 26.6091\n",
      "ROUGEL    : 42.1059\n",
      "\n",
      "Metrics for resolved input:\n",
      "Loss      : 1.4980\n",
      "ROUGE1    : 50.0951\n",
      "ROUGE2    : 26.9741\n",
      "ROUGEL    : 42.2931\n"
     ]
    }
   ],
   "source": [
    "metrics_map = {\n",
    "    \"eval_loss\":  \"Loss\",\n",
    "    \"eval_rouge1\": \"ROUGE1\",\n",
    "    \"eval_rouge2\": \"ROUGE2\",\n",
    "    \"eval_rougeL\": \"ROUGEL\",\n",
    "}\n",
    "\n",
    "for version, metrics in all_metrics.items():\n",
    "    print(f\"\\nMetrics for {version} input:\")\n",
    "    for key in metrics_map:\n",
    "        if key in metrics:\n",
    "            print(f\"{metrics_map[key]:<10}: {metrics[key]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zwTBx-KutAU"
   },
   "outputs": [],
   "source": [
    "raw_output = trainer_raw.predict(tokenized_data[\"validation\"])\n",
    "resolved_output = trainer_resolved.predict(tokenized_data[\"validation\"])\n",
    "\n",
    "raw_ids = raw_output.predictions\n",
    "if isinstance(raw_ids, tuple):\n",
    "    raw_ids = raw_ids[0]\n",
    "resolved_ids = resolved_output.predictions\n",
    "if isinstance(resolved_ids, tuple):\n",
    "    resolved_ids = resolved_ids[0]\n",
    "raw_ids = np.clip(raw_ids, 0, tokenizer.vocab_size - 1).astype(np.int64)\n",
    "resolved_ids = np.clip(resolved_ids, 0, tokenizer.vocab_size - 1).astype(np.int64)\n",
    "\n",
    "raw_outputs = tokenizer.batch_decode(raw_ids, skip_special_tokens=True)\n",
    "resolved_outputs = tokenizer.batch_decode(resolved_ids, skip_special_tokens=True)\n",
    "labels_ids = np.where(raw_output.label_ids!=-100, raw_output.label_ids, tokenizer.pad_token_id)\n",
    "refs = tokenizer.batch_decode(labels_ids,skip_special_tokens=True)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rouge1_raw, rouge2_raw, rougeL_raw = [], [], []\n",
    "rouge1_resolved, rouge2_resolved, rougeL_resolved = [], [], []\n",
    "for pred_raw, pred_resolved, ref in zip(raw_outputs, resolved_outputs, refs):\n",
    "    scores_raw = rouge.compute(predictions=[pred_raw], references=[ref])\n",
    "    scores_resolved = rouge.compute(predictions=[pred_resolved], references=[ref])\n",
    "    rouge1_raw.append(scores_raw[\"rouge1\"])\n",
    "    rouge2_raw.append(scores_raw[\"rouge2\"])\n",
    "    rougeL_raw.append(scores_raw[\"rougeL\"])\n",
    "    rouge1_resolved.append(scores_resolved[\"rouge1\"])\n",
    "    rouge2_resolved.append(scores_resolved[\"rouge2\"])\n",
    "    rougeL_resolved.append(scores_resolved[\"rougeL\"])\n",
    "rouge1_raw = np.array(rouge1_raw)\n",
    "rouge1_resolved = np.array(rouge1_resolved)\n",
    "rouge2_raw = np.array(rouge2_raw)\n",
    "rouge2_resolved = np.array(rouge2_resolved)\n",
    "rougeL_raw = np.array(rougeL_raw)\n",
    "rougeL_resolved = np.array(rougeL_resolved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1750045740718,
     "user": {
      "displayName": "YI-CHUN LO",
      "userId": "03461610637064647812"
     },
     "user_tz": -60
    },
    "id": "3UzAWf7YGPYJ",
    "outputId": "6bf2e778-9381-4fc6-c65f-6f8a188772d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Wilcoxon statistic = 96300.00  p-value = 0.1102\n",
      "ROUGE-1 Paired t-test statistic = 1.59  p-value = 0.1116\n",
      "ROUGE-2 Wilcoxon statistic = 81613.00  p-value = 0.0519\n",
      "ROUGE-2 Paired t-test statistic = 2.09  p-value = 0.0369\n",
      "ROUGE-L Wilcoxon statistic = 97438.00  p-value = 0.0394\n",
      "ROUGE-L Paired t-test statistic = 2.07  p-value = 0.0386\n"
     ]
    }
   ],
   "source": [
    "for name, raw_, resolved_ in [(\"ROUGE-1\", rouge1_raw, rouge1_resolved), (\"ROUGE-2\", rouge2_raw, rouge2_resolved), (\"ROUGE-L\",rougeL_raw,rougeL_resolved)]:\n",
    "    wilcoxon_statistic, wilcoxon_pvalue = wilcoxon(resolved_, raw_, alternative=\"two-sided\")\n",
    "    t_statistic, t_pvalue = ttest_rel(resolved_, raw_, alternative=\"two-sided\")\n",
    "    print(f\"{name} Wilcoxon statistic = {wilcoxon_statistic:.2f}  p-value = {wilcoxon_pvalue:.4f}\")\n",
    "    print(f\"{name} Paired t-test statistic = {t_statistic:.2f}  p-value = {t_pvalue:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1750045740720,
     "user": {
      "displayName": "YI-CHUN LO",
      "userId": "03461610637064647812"
     },
     "user_tz": -60
    },
    "id": "HPhc3gEyU5dV"
   },
   "outputs": [],
   "source": [
    "def summary(sample_idx, data, model, tokenizer, model_resolved, tokenizer_resolved, device):\n",
    "    text = data[\"test\"][sample_idx][\"dialogue\"]\n",
    "    resolved_text = data[\"test\"][sample_idx][\"resolved_text\"]\n",
    "    reference = data[\"test\"][sample_idx][\"summary\"]\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    output = model.generate(**inputs, max_length=128)\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    inputs_resolved = tokenizer_resolved(resolved_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    output_resolved = model_resolved.generate(**inputs_resolved, max_length=128)\n",
    "    summary_resolved = tokenizer_resolved.decode(output_resolved[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Input\")\n",
    "    print(text)\n",
    "    print()\n",
    "    print(\"Summary from Raw Model\")\n",
    "    print(summary)\n",
    "    print()\n",
    "    print(\"Summary from Anaphora Resolution Model\")\n",
    "    print(summary_resolved)\n",
    "    print()\n",
    "    print(\"Reference Summary\")\n",
    "    print(reference)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1841,
     "status": "ok",
     "timestamp": 1750045742564,
     "user": {
      "displayName": "YI-CHUN LO",
      "userId": "03461610637064647812"
     },
     "user_tz": -60
    },
    "id": "LftiM-C6XnIb"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/bart-large-raw-finetuned\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/bart-large-raw-finetuned\").to(device)\n",
    "\n",
    "tokenizer_resolved = AutoTokenizer.from_pretrained(\"/content/bart-large-resolved-finetuned\")\n",
    "model_resolved = AutoModelForSeq2SeqLM.from_pretrained(\"/content/bart-large-resolved-finetuned\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 998,
     "status": "ok",
     "timestamp": 1750045743567,
     "user": {
      "displayName": "YI-CHUN LO",
      "userId": "03461610637064647812"
     },
     "user_tz": -60
    },
    "id": "J8MF2eifa7WE",
    "outputId": "3b50de9d-bc8f-4327-c52e-8884f325b1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "Igor: Shit, I've got so much to do at work and I'm so demotivated. \r\n",
      "John: It's pretty irresponsible to give that much work to someone on their notice period.\r\n",
      "Igor: Yeah, exactly! Should I even care?\r\n",
      "John: It's up to you, but you know what they say...\r\n",
      "Igor: What do you mean?\r\n",
      "John: Well, they say how you end things shows how you really are...\r\n",
      "Igor: And now how you start, right?\r\n",
      "John: Gotcha! \r\n",
      "Igor: So what shall I do then? \r\n",
      "John: It's only two weeks left, so grit your teeth and do what you have to do. \r\n",
      "Igor: Easy to say, hard to perform.\r\n",
      "John: Come on, stop thinking, start doing! \r\n",
      "Igor: That's so typical of you!  ;)  \n",
      "\n",
      "Summary from Raw Model\n",
      "Igor has a lot of work to do at work. John reckons it's irresponsible to give so much work to someone on their notice period.\n",
      "\n",
      "Summary from Anaphora Resolution Model\n",
      "Igor has a lot of work to do at work. John reckons it's irresponsible to give too much work to someone on their notice period. Igor has two weeks left of notice.\n",
      "\n",
      "Reference Summary\n",
      "Igor has a lot of work on his notice period and he feels demotivated. John thinks he should do what he has to do nevertheless. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary(\n",
    "    sample_idx=17,\n",
    "    data=data,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_resolved=model_resolved,\n",
    "    tokenizer_resolved=tokenizer_resolved,\n",
    "    device=device\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPrURWfo8rFxHHURbVvaGkS",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
