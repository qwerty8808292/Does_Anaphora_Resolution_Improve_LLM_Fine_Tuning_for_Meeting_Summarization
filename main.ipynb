{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LfpZcsvKXzc_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    GenerationConfig,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        ")\n",
        "from transformers.generation.utils import EncoderDecoderCache\n",
        "import evaluate\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "from scipy.stats import wilcoxon, ttest_rel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pejXilMmX03-"
      },
      "outputs": [],
      "source": [
        "seed = 413\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fVPsdHy7YKx1"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4JSt0nSYLpl"
      },
      "outputs": [],
      "source": [
        "inputs = {\"raw\": \"dialogue\", \"resolved\": \"resolved_text\"}\n",
        "data = load_from_disk(\"samsum_new\")\n",
        "model_checkpoint = \"t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oQXgZqlcYc5V"
      },
      "outputs": [],
      "source": [
        "def preprocess(data, input):\n",
        "    inputs = data[input]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=data[\"summary\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F73Y-i5LYj0V"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "    predictions = np.array(predictions)\n",
        "    predictions = predictions.astype(np.int64)\n",
        "    predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n",
        "    predictions = predictions.tolist()\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    if isinstance(labels, tuple):\n",
        "        labels = labels[0]\n",
        "    labels = np.array(labels)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    labels = labels.astype(np.int64).tolist()\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    return {k: round(v * 100, 4) for k, v in result.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjAUa225YrNc"
      },
      "outputs": [],
      "source": [
        "all_metrics = OrderedDict()\n",
        "\n",
        "trainer_raw = None\n",
        "trainer_resolved = None\n",
        "\n",
        "\n",
        "for version, input_ in inputs.items():\n",
        "    tokenized_data = data.map(\n",
        "        lambda x: preprocess(x, input_),\n",
        "        batched=True,\n",
        "        remove_columns=data[\"train\"].column_names\n",
        "    )\n",
        "\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
        "    model.config.use_cache = False\n",
        "    for name, param in model.named_parameters():\n",
        "        if any(f\"encoder.block.{i}.\" in name for i in range(3)):\n",
        "            param.requires_grad = False\n",
        "\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=f\"/content/t5-base-{version}-finetuned\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        learning_rate=0.0001,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=1,\n",
        "        num_train_epochs=3,\n",
        "        predict_with_generate=True,\n",
        "        fp16=True,\n",
        "        push_to_hub=False,\n",
        "        logging_dir=f\"/content/logs-{version}\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        logging_steps=500,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "    \n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_data[\"train\"],\n",
        "        eval_dataset=tokenized_data[\"validation\"],\n",
        "        processing_class=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    if version == \"raw\":\n",
        "        trainer_raw = trainer\n",
        "    else:\n",
        "        trainer_resolved = trainer\n",
        "    trainer.train()\n",
        "    trainer.save_model(f\"/content/t5-base-{version}-finetuned\")\n",
        "\n",
        "    all_metrics[version] = trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bT81fhcdUiR-",
        "outputId": "bbba719a-8853-4ef3-b667-63e16961b6fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Metrics for raw input:\n",
            "Loss      : 2.8259\n",
            "ROUGE1    : 27.3721\n",
            "ROUGE2    : 9.4496\n",
            "ROUGEL    : 23.9784\n",
            "\n",
            "Metrics for resolved input:\n",
            "Loss      : 1.6585\n",
            "ROUGE1    : 41.3422\n",
            "ROUGE2    : 18.2445\n",
            "ROUGEL    : 34.6729\n"
          ]
        }
      ],
      "source": [
        "metrics_map = {\n",
        "    \"eval_loss\":  \"Loss\",\n",
        "    \"eval_rouge1\": \"ROUGE1\",\n",
        "    \"eval_rouge2\": \"ROUGE2\",\n",
        "    \"eval_rougeL\": \"ROUGEL\",\n",
        "}\n",
        "\n",
        "for version, metrics in all_metrics.items():\n",
        "    print(f\"\\nMetrics for {version} input:\")\n",
        "    for key in metrics_map:\n",
        "        if key in metrics:\n",
        "            print(f\"{metrics_map[key]:<10}: {metrics[key]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zwTBx-KutAU"
      },
      "outputs": [],
      "source": [
        "raw_output = trainer_raw.predict(tokenized_data[\"validation\"])\n",
        "resolved_output = trainer_resolved.predict(tokenized_data[\"validation\"])\n",
        "\n",
        "raw_ids = raw_output.predictions\n",
        "if isinstance(raw_ids, tuple):\n",
        "    raw_ids = raw_ids[0]\n",
        "resolved_ids = resolved_output.predictions\n",
        "if isinstance(resolved_ids, tuple):\n",
        "    resolved_ids = resolved_ids[0]\n",
        "raw_ids = np.clip(raw_ids, 0, tokenizer.vocab_size - 1).astype(np.int64)\n",
        "resolved_ids = np.clip(resolved_ids, 0, tokenizer.vocab_size - 1).astype(np.int64)\n",
        "\n",
        "raw_outputs = tokenizer.batch_decode(raw_ids, skip_special_tokens=True)\n",
        "resolved_outputs = tokenizer.batch_decode(resolved_ids, skip_special_tokens=True)\n",
        "labels_ids = np.where(raw_output.label_ids!=-100, raw_output.label_ids, tokenizer.pad_token_id)\n",
        "refs = tokenizer.batch_decode(labels_ids,skip_special_tokens=True)\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge1_raw, rouge2_raw, rougeL_raw = [], [], []\n",
        "rouge1_resolved, rouge2_resolved, rougeL_resolved = [], [], []\n",
        "for pred_raw, pred_resolved, ref in zip(raw_outputs, resolved_outputs, refs):\n",
        "    scores_raw = rouge.compute(predictions=[pred_raw], references=[ref])\n",
        "    scores_resolved = rouge.compute(predictions=[pred_resolved], references=[ref])\n",
        "    rouge1_raw.append(scores_raw[\"rouge1\"])\n",
        "    rouge2_raw.append(scores_raw[\"rouge2\"])\n",
        "    rougeL_raw.append(scores_raw[\"rougeL\"])\n",
        "    rouge1_resolved.append(scores_resolved[\"rouge1\"])\n",
        "    rouge2_resolved.append(scores_resolved[\"rouge2\"])\n",
        "    rougeL_resolved.append(scores_resolved[\"rougeL\"])\n",
        "rouge1_raw = np.array(rouge1_raw)\n",
        "rouge1_resolved = np.array(rouge1_resolved)\n",
        "rouge2_raw = np.array(rouge2_raw)\n",
        "rouge2_resolved = np.array(rouge2_resolved)\n",
        "rougeL_raw = np.array(rougeL_raw)\n",
        "rougeL_resolved = np.array(rougeL_resolved)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3UzAWf7YGPYJ",
        "outputId": "215c0806-a11a-4d73-af31-50ea8d6c794f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE-1 Wilcoxon statistic = 32264.50  p-value = 0.0000\n",
            "ROUGE-1 Paired t-test statistic = 23.06  p-value = 0.0000\n",
            "ROUGE-2 Wilcoxon statistic = 37346.00  p-value = 0.0000\n",
            "ROUGE-2 Paired t-test statistic = 15.58  p-value = 0.0000\n",
            "ROUGE-L Wilcoxon statistic = 44085.00  p-value = 0.0000\n",
            "ROUGE-L Paired t-test statistic = 19.10  p-value = 0.0000\n"
          ]
        }
      ],
      "source": [
        "for name, raw_, resolved_ in [(\"ROUGE-1\", rouge1_raw, rouge1_resolved), (\"ROUGE-2\", rouge2_raw, rouge2_resolved), (\"ROUGE-L\",rougeL_raw,rougeL_resolved)]:\n",
        "    wilcoxon_statistic, wilcoxon_pvalue = wilcoxon(resolved_, raw_, alternative=\"two-sided\")\n",
        "    t_statistic, t_pvalue = ttest_rel(resolved_, raw_, alternative=\"two-sided\")\n",
        "    print(f\"{name} Wilcoxon statistic = {wilcoxon_statistic:.2f}  p-value = {wilcoxon_pvalue:.4f}\")\n",
        "    print(f\"{name} Paired t-test statistic = {t_statistic:.2f}  p-value = {t_pvalue:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HPhc3gEyU5dV"
      },
      "outputs": [],
      "source": [
        "def summary(sample_idx, data, model, tokenizer, model_resolved, tokenizer_resolved, device):\n",
        "    text = data[\"test\"][sample_idx][\"dialogue\"]\n",
        "    resolved_text = data[\"test\"][sample_idx][\"resolved_text\"]\n",
        "    reference = data[\"test\"][sample_idx][\"summary\"]\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "    output = model.generate(**inputs, max_length=128)\n",
        "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    inputs_resolved = tokenizer_resolved(resolved_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "    output_resolved = model_resolved.generate(**inputs_resolved, max_length=128)\n",
        "    summary_resolved = tokenizer_resolved.decode(output_resolved[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"Input\")\n",
        "    print(text)\n",
        "    print()\n",
        "    print(\"Summary from Raw Model\")\n",
        "    print(summary)\n",
        "    print()\n",
        "    print(\"Summary from Anaphora Resolution Model\")\n",
        "    print(summary_resolved)\n",
        "    print()\n",
        "    print(\"Reference Summary\")\n",
        "    print(reference)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LftiM-C6XnIb"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/t5-base-raw-finetuned\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/t5-base-raw-finetuned\").to(device)\n",
        "\n",
        "tokenizer_resolved = AutoTokenizer.from_pretrained(\"/content/t5-base-resolved-finetuned\")\n",
        "model_resolved = AutoModelForSeq2SeqLM.from_pretrained(\"/content/t5-base-resolved-finetuned\").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "J8MF2eifa7WE",
        "outputId": "d5dbd4bd-66f5-4de7-fcf4-7f0d99f04f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input\n",
            "Igor: Shit, I've got so much to do at work and I'm so demotivated. \r\n",
            "John: It's pretty irresponsible to give that much work to someone on their notice period.\r\n",
            "Igor: Yeah, exactly! Should I even care?\r\n",
            "John: It's up to you, but you know what they say...\r\n",
            "Igor: What do you mean?\r\n",
            "John: Well, they say how you end things shows how you really are...\r\n",
            "Igor: And now how you start, right?\r\n",
            "John: Gotcha! \r\n",
            "Igor: So what shall I do then? \r\n",
            "John: It's only two weeks left, so grit your teeth and do what you have to do. \r\n",
            "Igor: Easy to say, hard to perform.\r\n",
            "John: Come on, stop thinking, start doing! \r\n",
            "Igor: That's so typical of you!  ;)  \n",
            "\n",
            "Summary from Raw Model\n",
            "John has been working for his job.\n",
            "\n",
            "Summary from Anaphora Resolution Model\n",
            "Igor has two weeks left to finish work.\n",
            "\n",
            "Reference Summary\n",
            "Igor has a lot of work on his notice period and he feels demotivated. John thinks he should do what he has to do nevertheless. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "summary(\n",
        "    sample_idx=17,\n",
        "    data=data,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    model_resolved=model_resolved,\n",
        "    tokenizer_resolved=tokenizer_resolved,\n",
        "    device=device\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
